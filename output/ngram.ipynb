{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gramを利用して変数名の提案"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ライブラリーと関数の宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_graphs(graph, graphs):\n",
    "  for search in graphs:\n",
    "    if graph.graph == search.graph:\n",
    "      return search\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_node(raw_doc):\n",
    "  data = []\n",
    "  sub_data = ''\n",
    "  for i in range(len(raw_doc)): \n",
    "    sub_data = raw_doc[i][0]\n",
    "    for j in range(1, len(raw_doc[i])):\n",
    "      sub_data = sub_data + ' X ' + raw_doc[i][j]\n",
    "    data.append(sub_data)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def similar(test_vec, test_dict:dict, graphs, topn = 10):\n",
    "  dists = []\n",
    "  index_to_key = []\n",
    "  i = 0\n",
    "  result = []\n",
    "  for key in test_dict.keys():\n",
    "    dist = cosine_similarity(test_vec.reshape(1, -1), test_dict[key].reshape(1, -1))\n",
    "    index_to_key.append(key)\n",
    "    dists.append(float(dist))\n",
    "\n",
    "\n",
    "  index_to_key = np.array(index_to_key)\n",
    "  dists = np.array(dists) \n",
    "  best = dists.argsort()[::-1] \n",
    "  for sim in best:\n",
    "    try:\n",
    "      result.append([index_to_key[sim], graphs[sim].graph['label1'], graphs[sim].graph['label2'], float(dists[sim])])\n",
    "    except KeyError:\n",
    "      print(sim)\n",
    "      continue\n",
    "  return result[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_combine(sim_list):\n",
    "  result = {}\n",
    "  result_count ={}\n",
    "  sim = 0\n",
    "  for i in range(len(sim_list)):\n",
    "    try:\n",
    "      if sim_list[i][2] in result[sim_list[i][1]].keys():\n",
    "        continue\n",
    "    except KeyError:\n",
    "      result[sim_list[i][1]] = {}\n",
    "      result_count[sim_list[i][1]] = {}\n",
    "    temp = sim_list[i]\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    for j in range(i,len(sim_list)):\n",
    "      if temp[2] == sim_list[j][2] and temp[1] == sim_list[j][1]:\n",
    "        sum = sum + sim_list[j][3]\n",
    "        count += 1\n",
    "    avg = sum/count\n",
    "    result[sim_list[i][1]].update({sim_list[i][2] : avg})\n",
    "    result_count[sim_list[i][1]].update({sim_list[i][2] : count})\n",
    "  return result, result_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyWeisfeilerLehaman import WeisfeilerLehmanHashing\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import numpy as np\n",
    "\n",
    "def WLchange(graphs, wl_iterations, attributed, erase_base_features):\n",
    "    documents = [\n",
    "    WeisfeilerLehmanHashing(\n",
    "        graph, wl_iterations, attributed, erase_base_features\n",
    "    )\n",
    "    for graph in graphs\n",
    "    ]\n",
    "\n",
    "    documents = [\n",
    "        doc.get_graph_features() for i, doc in enumerate(documents)\n",
    "    ]\n",
    "    return documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練ファイルの入力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9993"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loopfile = glob.glob('/Users/wintor7/Research/loopset/*.gpickle', recursive=True)\n",
    "graphfile = glob.glob('/Users/wintor7/Research/graphset_new/*.gpickle', recursive=True)\n",
    "\n",
    "graphs = []\n",
    "for i in range(int(len(graphfile)/10)):\n",
    "  G = nx.read_gpickle(graphfile[i])\n",
    "  graphs.append(G)\n",
    "loop_graphs = []\n",
    "for i in range(int(len(loopfile)/10)):\n",
    "  G = nx.read_gpickle(loopfile[i])\n",
    "  loop_graphs.append(G)\n",
    "\n",
    "graphfile = glob.glob(\"/Users/wintor7/report7/graph/var/*.gpickle\")\n",
    "loopfile = glob.glob(\"/Users/wintor7/report7/graph/loop/*.gpickle\")\n",
    "\n",
    "\n",
    "\n",
    "for gfile in graphfile:\n",
    "  G = nx.read_gpickle(gfile)\n",
    "  graphs.append(G)\n",
    "\n",
    "\n",
    "for i in loopfile:\n",
    "  G = nx.read_gpickle(i)\n",
    "  loop_graphs.append(G)\n",
    "\n",
    "len(graphs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抽出範囲拡大部分の追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = WLchange(graphs, 2, True, False)\n",
    "for i in range(len(graphs)):\n",
    "    loop = match_graphs(graphs[i], loop_graphs)\n",
    "    documents[i].append('loop')\n",
    "    if loop:\n",
    "        loop = WeisfeilerLehmanHashing(loop, 2, True, False)\n",
    "        loop = loop.get_graph_features()\n",
    "        for j in range(len(loop)):\n",
    "            documents[i].append(loop[j])\n",
    "    else:\n",
    "        documents[i].append('Null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_withtag = [\n",
    "    TaggedDocument(words=doc, tags=[str(i)])\n",
    "    for i, doc in enumerate(documents)\n",
    "]\n",
    "\n",
    "raw_doc = []\n",
    "for i in range(len(doc_withtag)):\n",
    "  raw_doc.append(doc_withtag[i].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combine_node(raw_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29 X 29 49 X 29 49 49 51 57 59 66 X 49 X 49 51 57 59 66 X 49 51 57 59 66 51 57 57 None 59 59 75 66 75 X 59 X 59 59 75 X 59 59 75 59 66 75 75 None X 59 X 59 66 75 X 59 66 75 66 75 75 None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 75 X 75 None X 75 None None X 75 X 75 None X 75 None None X 51 X 51 57 X 51 57 57 None X 57 X 57 None X 57 None None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 57 X 57 None X 57 None None X loop X Null'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "token_pattern = r'\\b\\w+\\b'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wintor7/miniconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ngram=2\n",
    "vectorizer = CountVectorizer(ngram_range=(1,ngram), token_pattern=token_pattern, min_df=1)\n",
    "\n",
    "X = vectorizer.fit(data)\n",
    "\n",
    "vocab_2 = vectorizer.get_feature_names()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抽象構文木の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Mygraph2vec import Graph2Vec\n",
    "import networkx as nx\n",
    "from ast_draw import CodeTransformer, counter\n",
    "import glob\n",
    "import ast\n",
    "import node_to_vec as n2v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストファイルの入力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/wintor7/report7/test_file/e205760_search_words_sub_set.py',\n",
       " '/Users/wintor7/report7/test_file/e215701_search_words_intermediate_result.py',\n",
       " '/Users/wintor7/report7/test_file/e205715_search_words_searched.py',\n",
       " '/Users/wintor7/report7/test_file/e195760_search_words_search_word.py',\n",
       " '/Users/wintor7/report7/test_file/e195743_search_words_one_youso_list.py',\n",
       " '/Users/wintor7/report7/test_file/e215710_search_words_words_dict.py',\n",
       " '/Users/wintor7/report7/test_file/e205708_search_words_save_files.py',\n",
       " '/Users/wintor7/report7/test_file/e195735_search_words_result.py',\n",
       " '/Users/wintor7/report7/test_file/e215703_search_words_count.py',\n",
       " '/Users/wintor7/report7/test_file/e195730_search_words_file_list.py',\n",
       " '/Users/wintor7/report7/test_file/e195763_search_words_result_files.py',\n",
       " '/Users/wintor7/report7/test_file/e205728_search_words_items_.py',\n",
       " '/Users/wintor7/report7/test_file/e215716_search_words_count.py',\n",
       " '/Users/wintor7/report7/test_file/e205717_search_words_words.py',\n",
       " '/Users/wintor7/report7/test_file/e215706_search_words_kekka.py',\n",
       " '/Users/wintor7/report7/test_file/e185708_search_words_stored_file.py']"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "path = '/Users/wintor7/report7/test_file'\n",
    "file = glob.glob(path + '/**_search_words_**.py')\n",
    "func = 'search_words'\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/wintor7/report7/test_file/e205717_search_words_words.py'"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "var, for_var, func_var, func_params, func_return = {}, {}, {}, {}, {}\n",
    "dict_func, var_counter = {},{}\n",
    "node_to_id = {}\n",
    "id_to_node = {}\n",
    "node_to_id, id_to_node = n2v.create_node_lib()      #create node corpus\n",
    "var_to_id, func_to_id = {},{}\n",
    "id_to_var, id_to_func = {},{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_var = 'test_var1'\n",
    "with open(file[-3], 'r') as fh:\n",
    "  content = fh.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_node = ast.parse(content)\n",
    "\n",
    "transformer = CodeTransformer()\n",
    "r_node = transformer.visit(r_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'make_index': {'index': [<_ast.Assign at 0x130596760>,\n",
       "   <_ast.Assign at 0x1304b45b0>,\n",
       "   <_ast.Return at 0x1307f80d0>],\n",
       "  'sentence': [<_ast.Assign at 0x130560b50>,\n",
       "   <_ast.Expr at 0x1307489a0>,\n",
       "   <_ast.For at 0x1307489d0>,\n",
       "   <_ast.For at 0x1304b4940>,\n",
       "   <_ast.If at 0x1304b42e0>],\n",
       "  'filenames': [<_ast.For at 0x1305607f0>, <_ast.Expr at 0x1304b4fa0>],\n",
       "  'fh': [<_ast.With at 0x130748250>, <_ast.Expr at 0x1307489a0>],\n",
       "  'names': [<_ast.Assign at 0x1304b4340>,\n",
       "   <_ast.Expr at 0x1304b4fa0>,\n",
       "   <_ast.Assign at 0x1304b45b0>]},\n",
       " 'search_words': {'test_var': [<_ast.Assign at 0x1307f83a0>,\n",
       "   <_ast.Assign at 0x1307f8700>,\n",
       "   <_ast.If at 0x1307f8790>,\n",
       "   <_ast.Return at 0x1307f88b0>],\n",
       "  'index': [<_ast.Assign at 0x1307f83a0>, <_ast.Assign at 0x1307f8700>],\n",
       "  'test_var1': [<_ast.Assign at 0x1307f83a0>, <_ast.For at 0x1307f8670>]}}"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_func = counter(dict_func, transformer.var_in_func)\n",
    "\n",
    "name_line = transformer.name_line\n",
    "node_list = {}\n",
    "\n",
    "for key in name_line.keys():\n",
    "  var_node = {}\n",
    "  for var, value in name_line[key].items():\n",
    "    for i in value:\n",
    "      transformer.track_line(r_node, i)\n",
    "      if var not in var_node.keys():\n",
    "        var_node[var] = [transformer.target_var]\n",
    "      elif transformer.target_var not in var_node[var]:\n",
    "        var_node[var].append(transformer.target_var)\n",
    "  node_list[key] = var_node\n",
    "node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'make_index': {'index': [None, <_ast.For at 0x1307489d0>],\n",
       "  'sentence': [<_ast.For at 0x1307489d0>, <_ast.For at 0x1305607f0>],\n",
       "  'filenames': [<_ast.For at 0x1305607f0>, <_ast.For at 0x1307489d0>],\n",
       "  'fh': [<_ast.For at 0x1305607f0>],\n",
       "  'names': [<_ast.For at 0x1307489d0>]},\n",
       " 'search_words': {'test_var': [<_ast.Try at 0x1307f82e0>],\n",
       "  'index': [<_ast.Try at 0x1307f82e0>],\n",
       "  'test_var1': [<_ast.Try at 0x1307f82e0>]}}"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_list = {}        #\n",
    "for key in name_line.keys():\n",
    "  loop_temp_list = {}\n",
    "  for var, value in name_line[key].items():\n",
    "    for i in value:\n",
    "      transformer.track_loop(r_node,i)\n",
    "      if var not in loop_temp_list.keys():\n",
    "        loop_temp_list[var] = [transformer.target_loop]\n",
    "      elif transformer.target_loop not in loop_temp_list[var]:\n",
    "        loop_temp_list[var].append(transformer.target_loop)\n",
    "  loop_list[key] = loop_temp_list \n",
    "loop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_list = list(transformer.var_in_func.keys())\n",
    "var_list = []\n",
    "for i,v in enumerate(transformer.var_in_func):\n",
    "  for i in transformer.var_in_func[v]:\n",
    "    var_list.append(i)\n",
    "var_to_id, id_to_var = n2v.name_corpus(var_list, var_to_id, id_to_var)\n",
    "func_to_id, id_to_func = n2v.name_corpus(func_list, func_to_id, id_to_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list = []\n",
    "graph = nx.DiGraph()\n",
    "str_to_node = {}\n",
    "for var_node in node_list[func][test_var]:\n",
    "  root = len(str_to_node)\n",
    "  n2v.create_graph(var_node, node_to_id, graph, str_to_node, root)\n",
    "  for node in graph.nodes():\n",
    "    graph.nodes[node]['feature'] = str_to_node[node]\n",
    "  root += 1  \n",
    "graph_list.append(graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_loop = []\n",
    "graph = nx.DiGraph()\n",
    "str_to_node = {}\n",
    "for var_node in loop_list[func][test_var]:\n",
    "  if var_node == None:\n",
    "    continue\n",
    "  root = len(str_to_node)\n",
    "  n2v.create_graph(var_node, node_to_id, graph, str_to_node, root)\n",
    "  for node in graph.nodes():\n",
    "    graph.nodes[node]['feature'] = str_to_node[node]\n",
    "  root += 1  \n",
    "graphs_loop.append(graph) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WL再ラベル操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "  WeisfeilerLehmanHashing(\n",
    "      graph, 2, True, False)\n",
    "  for graph in graph_list\n",
    "]\n",
    "\n",
    "\n",
    "documents_loop = [\n",
    "  WeisfeilerLehmanHashing(\n",
    "      graph, 2, True, False)\n",
    "  for graph in graphs_loop\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "  doc.get_graph_features() for i, doc in enumerate(documents)\n",
    "]\n",
    "\n",
    "documents_loop = [\n",
    "  doc.get_graph_features() for i, doc in enumerate(documents_loop)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].append('loop')\n",
    "if documents_loop != None:\n",
    "  for j in range(len(documents_loop[0])):\n",
    "    documents[0].append(documents_loop[0][j])\n",
    "else:\n",
    "  documents[0].append('Null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = combine_node(documents)\n",
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13 X 13 49 66 X 13 49 66 49 64 66 66 76 X 66 X 66 76 X 66 76 76 None X 76 X 76 None X 76 None None X 49 X 49 64 66 X 49 64 66 64 63 66 75 66 75 X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 64 X 64 63 66 75 X 64 63 66 75 63 64 66 75 75 None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 63 X 63 64 X 63 64 64 63 66 75 X 64 X 64 63 66 75 X 64 63 66 75 63 57 66 75 75 None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 63 X 63 57 X 63 57 57 None X 57 X 57 None X 57 None None X 75 X 75 None X 75 None None X 75 X 75 None X 75 None None X 16 X 16 13 66 66 X 16 13 66 66 13 35 66 66 75 66 76 X 66 X 66 76 X 66 76 76 None X 76 X 76 None X 76 None None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 13 X 13 35 66 X 13 35 66 35 49 66 82 66 76 X 66 X 66 76 X 66 76 76 None X 76 X 76 None X 76 None None X 35 X 35 49 66 82 X 35 49 66 82 49 64 66 66 75 82 None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 82 X 82 None X 82 None None X 49 X 49 64 66 X 49 64 66 64 63 66 75 66 75 X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 64 X 64 63 66 75 X 64 63 66 75 63 66 66 75 75 None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 63 X 63 66 X 63 66 66 75 X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 75 X 75 None X 75 None None X loop X 23 X 23 112 13 16 19 X 23 112 13 16 19 112 11 68 13 49 66 16 13 66 66 19 11 11 48 X 13 X 13 49 66 X 13 49 66 49 64 66 66 76 X 66 X 66 76 X 66 76 76 None X 76 X 76 None X 76 None None X 49 X 49 64 66 X 49 64 66 64 63 66 75 66 75 X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 64 X 64 63 66 75 X 64 63 66 75 63 64 66 75 75 None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 63 X 63 64 X 63 64 64 63 66 75 X 64 X 64 63 66 75 X 64 63 66 75 63 57 66 75 75 None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 63 X 63 57 X 63 57 57 None X 57 X 57 None X 57 None None X 75 X 75 None X 75 None None X 75 X 75 None X 75 None None X 16 X 16 13 66 66 X 16 13 66 66 13 35 66 66 75 66 76 X 66 X 66 76 X 66 76 76 None X 76 X 76 None X 76 None None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 13 X 13 35 66 X 13 35 66 35 49 66 82 66 76 X 66 X 66 76 X 66 76 76 None X 76 X 76 None X 76 None None X 35 X 35 49 66 82 X 35 49 66 82 49 64 66 66 75 82 None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 82 X 82 None X 82 None None X 49 X 49 64 66 X 49 64 66 64 63 66 75 66 75 X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 64 X 64 63 66 75 X 64 63 66 75 63 66 66 75 75 None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 63 X 63 66 X 63 66 66 75 X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 75 X 75 None X 75 None None X 19 X 19 11 11 48 X 19 11 11 48 11 49 11 57 48 108 49 57 X 48 X 48 108 49 57 X 48 108 49 57 108 None 49 66 66 57 None X 49 X 49 66 66 X 49 66 66 66 75 66 75 X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 108 X 108 None X 108 None None X 57 X 57 None X 57 None None X 11 X 11 49 X 11 49 49 49 66 X 49 X 49 49 66 X 49 49 66 49 66 66 66 75 X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 49 X 49 66 66 X 49 66 66 66 75 66 75 X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 11 X 11 57 X 11 57 57 None X 57 X 57 None X 57 None None X 112 X 112 11 68 X 112 11 68 11 57 68 66 66 75 X 68 X 68 66 66 75 X 68 66 66 75 66 75 66 75 75 None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 66 X 66 75 X 66 75 75 None X 75 X 75 None X 75 None None X 75 X 75 None X 75 None None X 11 X 11 57 X 11 57 57 None X 57 X 57 None X 57 None None']"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>108</th>\n",
       "      <th>108 49</th>\n",
       "      <th>108 none</th>\n",
       "      <th>108 x</th>\n",
       "      <th>11</th>\n",
       "      <th>11 11</th>\n",
       "      <th>11 48</th>\n",
       "      <th>11 49</th>\n",
       "      <th>11 57</th>\n",
       "      <th>11 68</th>\n",
       "      <th>...</th>\n",
       "      <th>x 49</th>\n",
       "      <th>x 57</th>\n",
       "      <th>x 63</th>\n",
       "      <th>x 64</th>\n",
       "      <th>x 66</th>\n",
       "      <th>x 68</th>\n",
       "      <th>x 75</th>\n",
       "      <th>x 76</th>\n",
       "      <th>x 82</th>\n",
       "      <th>x loop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   108  108 49  108 none  108 x  11  11 11  11 48  11 49  11 57  11 68  ...  \\\n",
       "0    7       3         3      1  21      3      3      3      6      3  ...   \n",
       "\n",
       "   x 49  x 57  x 63  x 64  x 66  x 68  x 75  x 76  x 82  x loop  \n",
       "0    21    15    18    18    87     3    90    18     6       1  \n",
       "\n",
       "[1 rows x 125 columns]"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram=2\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,ngram), token_pattern=token_pattern, min_df=1)\n",
    "test_vec = vectorizer.fit_transform(test_data)\n",
    "vocab_test = vectorizer.get_feature_names_out()\n",
    "test_vectors = test_vec.toarray()\n",
    "df = pd.DataFrame(test_vectors, columns=vocab_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vocab_test)):\n",
    "  if vocab_test[i] not in vocab_2:\n",
    "    print(i)\n",
    "    vocab_2.append(vocab_test[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = {}\n",
    "ngram=2\n",
    "vectorizer = CountVectorizer(ngram_range=(1,ngram), token_pattern=token_pattern, min_df=1 , vocabulary= vocab_2)\n",
    "X = vectorizer.fit_transform(data)\n",
    "raw_vectors = X.toarray()\n",
    "#df = pd.DataFrame(raw_vectors, columns=vocab_2)\n",
    "for i in range(len(raw_vectors)):\n",
    "  train_vec[doc_withtag[i].tags[0]] = raw_vectors[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = vectorizer.fit_transform(test_data)\n",
    "vocab_test = vectorizer.get_feature_names_out()\n",
    "test_vectors = test_vec.toarray()\n",
    "test_vectors = test_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91514691]])"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cosine_similarity(train_vec['0'].reshape(1, -1),test_vectors.reshape(1, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2092,)"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectors.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "類似度の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = similar(test_vectors,train_vec,graphs,topn=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gramにより提案結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'search_words': {'keywords_for_search': 0.9989986027312507, 'names': 0.9975165214738414, 'search': 0.9946123750485458, 'index': 0.9928625724241522, 'result': 0.9936409412552768, 'words': 0.9934546389646055, 'keywords': 0.995183190065394, 'file_list': 0.9930008192434482, 'search_word_list': 0.9943278611479192, 'for_file': 0.9936352050983485, 'filename': 0.9931566876163171, 'word': 0.992617261024483, 'result_file': 0.9925970338710742, 'search_result': 0.9923168293807022, 'in_file': 0.9922135057783251, 'search_list': 0.9918969568527943}, 'dfs': {'max_size': 0.995646003900327}, 'check_location_is_safe': {'arr': 0.9928592809599277}, 'slash': {'var': 0.9926192472897601}, 'make_index': {'filename': 0.9924209461182144}}\n",
      "{'search_words': {'keywords_for_search': 7, 'names': 6, 'search': 13, 'index': 17, 'result': 5, 'words': 8, 'keywords': 1, 'file_list': 11, 'search_word_list': 1, 'for_file': 7, 'filename': 1, 'word': 3, 'result_file': 7, 'search_result': 3, 'in_file': 3, 'search_list': 3}, 'dfs': {'max_size': 1}, 'check_location_is_safe': {'arr': 1}, 'slash': {'var': 1}, 'make_index': {'filename': 1}}\n"
     ]
    }
   ],
   "source": [
    "result, result_count = sim_combine(result)\n",
    "print(result)\n",
    "print(result_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "課題文とのマッチング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_alternateName as ga"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マッチングにより提案結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['element']"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_list = ga.get_altName(result[func])\n",
    "all_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d61d1a6ce7c2368d95644e80c49b86bd9c88a962e5c89c961b90ebaea98c1f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
